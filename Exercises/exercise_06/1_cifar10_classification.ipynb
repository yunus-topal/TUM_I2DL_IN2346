{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g6vUCiE_N5eU"
   },
   "source": [
    "# Cifar10 Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X9ridWdTN5eY"
   },
   "source": [
    "Until now, we have implemented several pieces of a deep learning pipeline and trained a two-layer neural network, but all hyperparameters were pre-set to values yielding resonable results. However, in real problems a large part of the work will be geared towards finding the best hyperparameter settings for a certain problem. In this notebook we will explore some good practices for network debugging and hyperparameters search, as well as extending our binary classification neural network to a multi-class one.\n",
    "\n",
    "Let's go!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4sTNILP7OCzb"
   },
   "source": [
    "## (Optional) Mount folder in Colab\n",
    "\n",
    "Uncomment the following cell to mount your gdrive if you are using the notebook in google colab:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 35664,
     "status": "ok",
     "timestamp": 1650011700447,
     "user": {
      "displayName": "Weber Anna",
      "userId": "18154618360144454414"
     },
     "user_tz": -120
    },
    "id": "PoG-5okpOPht",
    "outputId": "a670ffa3-ee59-4ef5-e323-3f3fec373486"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "from google.colab import drive\n",
    "import os\n",
    "\n",
    "gdrive_path='/content/gdrive/MyDrive/i2dl/exercise_06'\n",
    "\n",
    "# This will mount your google drive under 'MyDrive'\n",
    "drive.mount('/content/gdrive', force_remount=True)\n",
    "# In order to access the files in this notebook we have to navigate to the correct folder\n",
    "os.chdir(gdrive_path)\n",
    "# Check manually if all files are present\n",
    "print(sorted(os.listdir()))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JZmN1ATIN5eY"
   },
   "outputs": [],
   "source": [
    "# Some lengthy setup.\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import urllib.request\n",
    "\n",
    "from exercise_code.networks.layer import (\n",
    "    Sigmoid, \n",
    "    Relu, \n",
    "    LeakyRelu, \n",
    "    Tanh,\n",
    ")\n",
    "from exercise_code.data import (\n",
    "    DataLoader,\n",
    "    ImageFolderDataset,\n",
    "    MemoryImageFolderDataset,\n",
    "    RescaleTransform,\n",
    "    NormalizeTransform,\n",
    "    FlattenTransform,\n",
    "    ComposeTransform,\n",
    ")\n",
    "from exercise_code.data.image_folder_dataset import RandomHorizontalFlip\n",
    "from exercise_code.networks import (\n",
    "    ClassificationNet,\n",
    "    BCE,\n",
    "    CrossEntropyFromLogits\n",
    ")\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tRdaBwnMN5ea"
   },
   "source": [
    "# 1. Quick recap (and some new things)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zsNrBTkyN5eb"
   },
   "source": [
    "Until now, in the previous exercises, we focused on building and understanding all the necessary modules for training a simple model. We followed the Pytorch implementations closely, as this is the framework we will use later and we wanted you to have a smoother transition to its APIs. \n",
    "\n",
    "In the figure below you can see the main components in Pytorch. Let's start with a quick recap of **our implementation** of these components. \n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "    <h3>Task: Check Code</h3>\n",
    "    <p>Everything is already implemented for this part, but we <b>strongly</b> encourage you to check  the respective source files in order to have a better understanding. </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "reTr9-VRN5ec"
   },
   "source": [
    "<img src=\"https://cdn-images-1.medium.com/max/800/1*uZrS4KjAuSJQIJPgOiaJUg.png\" style=\"width: 500px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4fZoL6hvN5ec"
   },
   "source": [
    "## 1.1 Dataset and Dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jvv1QqmJN5ed"
   },
   "source": [
    "Data preparation is an important part of deep learning projects. Because the data can come in different formats and from different sources, it must be prepared in a certain way, which depends on the application. One part, however, is uniform: since an entire dataset is usually too large to handle at once, we train our models on smaller batches of data. \n",
    "\n",
    "The goal of the ```Dataset``` class is to encapsulate all the 'dirty' data processing: loading and cleaning the data, storing features (or names of files where features can be found) and labels, as well as providing the means for accessing individual (transformed) items of the data using the ```__getitem__()``` function and an index. You already implemented an ```ImageFolderDataset``` (in ```exercise_code/data/image_folder_dataset.py```) class in Exercise 3. We we will reuse this class here.\n",
    "\n",
    "For processing the data, you implemented several transforms in Exercise 3 (```RescaleTransform```, ```NormalizeTransform```, ```ComposeTransform```). In this exercise we are working with images, which are multidimensional arrays, but we are using a simple feedforward neural network which takes an one dimensional array as an input, so it is necessary to reshape the images before feeding them into the model. \n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "    <h3>Task: Check Code</h3>\n",
    "    <p>Please check the implementation of the reshape operation in the <code>FlattenTransform</code> class, which can be found in <code>../exercise_06/exercise_code/data/image_folder_dataset.py</code>. </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_url = \"https://i2dl.vc.in.tum.de/static/data/cifar10.zip\"\n",
    "i2dl_exercises_path = os.path.dirname(os.path.abspath(os.getcwd()))\n",
    "cifar_root = os.path.join(i2dl_exercises_path, \"datasets\", \"cifar10\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The usual memory vs local data warning\n",
    "If you are using google colab or store your files on a local HDD, iterating over dataset takes quite some time and blablabla. You know the drill by now ;).\n",
    "\n",
    "<div class=\"alert alert-danger\">\n",
    "    <h3>Warning</h3>\n",
    "    <p>Loading the whole dataset into memory will not work if you are using a machine with 4GB of RAM or less (depending on your other programs such as memory hungry web browsers). Consider closing some open programs or simply use the local on-demand ImageFolderDataset.</p>\n",
    "    <p>In addition we want to warn you that everytime you execute a cell like \"dataset2 = MemoryImageFolderDataset...\" you are loading a 1.2GB matrix into your memory. If you do this often enough this notebook will crash on every machine. Therefore, we make sure to always use a single variable \"dataset\" which will be overwritten by future cells to avoid straining your memory too much.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose your preferred dataset here\n",
    "\n",
    "DATASET = ImageFolderDataset\n",
    "#DATASET = MemoryImageFolderDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that that is taken care of, back to the actual loading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2fHY8xTPN5ee"
   },
   "outputs": [],
   "source": [
    "# Use the Cifar10 mean and standard deviation computed in Exercise 3.\n",
    "cifar_mean = np.array([0.49191375, 0.48235852, 0.44673872])\n",
    "cifar_std  = np.array([0.24706447, 0.24346213, 0.26147554])\n",
    "\n",
    "# Define all the transforms we will apply on the images when \n",
    "# retrieving them.\n",
    "rescale_transform = RescaleTransform()\n",
    "normalize_transform = NormalizeTransform(\n",
    "    mean=cifar_mean,\n",
    "    std=cifar_std\n",
    ")\n",
    "flatten_transform = FlattenTransform()\n",
    "compose_transform = ComposeTransform([rescale_transform, \n",
    "                                      normalize_transform,\n",
    "                                      flatten_transform])\n",
    "\n",
    "# Create a train, validation and test dataset.\n",
    "datasets = {}\n",
    "for mode in ['train', 'val', 'test']:\n",
    "    crt_dataset = DATASET(\n",
    "        mode=mode,\n",
    "        root=cifar_root, \n",
    "        transform=compose_transform,\n",
    "        split={'train': 0.6, 'val': 0.2, 'test': 0.2}\n",
    "    )\n",
    "    datasets[mode] = crt_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FsR4dM2kN5ee"
   },
   "source": [
    "Then, based on this ```Dataset``` object, we can construct a ```Dataloader``` object which samples a random mini-batch of data at once. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3t4Xj_n3N5ef"
   },
   "outputs": [],
   "source": [
    "# Create a dataloader for each split.\n",
    "dataloaders = {}\n",
    "for mode in ['train', 'val', 'test']:\n",
    "    crt_dataloader = DataLoader(\n",
    "        dataset=datasets[mode],\n",
    "        batch_size=256,\n",
    "        shuffle=True,\n",
    "        drop_last=True,\n",
    "    )\n",
    "    dataloaders[mode] = crt_dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CpruSFLkN5ef"
   },
   "source": [
    "Because the ```Dataloader``` has the ```__iter__()``` method, we can simply iterate through the batches it produces, like this:\n",
    "\n",
    "```python\n",
    "for batch in dataloader['train']:\n",
    "    do_something(batch)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L81EGyslN5eg"
   },
   "source": [
    "## 1.2 Data Augmentation\n",
    "\n",
    "After the preprocessing steps, our data is in a good shape and ready to be fed into our network. As explained in the chapter above, we used the transformation functions `RescaleTransform`, `NormalizeTransform` and `FlattenTransform` to achieve this shape. These are the general steps that you need to perform on the data before you can start training. Of course, all these steps have to be applied to our three dataset splits (train, val and test split). In other words, preprocessing involves preparing the data before is can be used for training and inference. \n",
    "\n",
    "Besides these basic transformations, there are many other transformation methods that you can apply to the images. For example, you can <b>flip the images horizontally</b> or <b>blur the images</b> and use these new images to enlarge the dataset. This idea is called Data Augmentation and it involves methods that alter the training images to generate a synthetic dataset that is larger than your original dataset. This will hopefully improve the performance of your model. There is one important difference between data augmentation and data preprocessing: The transformation methods used to enlarge your dataset should only be applied to the training data. The validation and test data should not be affected by these methods.\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "    <h3>Task: Check Code</h3>\n",
    "    <p>The choice of transformation methods for data augmentation can be seen as a hyperparameter of your model. You can try to include these to enlarge your training data and obtain better results for your model. In <code>exercise_code/data/image_folder_dataset.py</code> we implemented the function <code>RandomHorizontalFlip</code> for you, which is randomly flipping an image horizontally. Check out the implementation.</p>\n",
    "    <p> Later, we will perform hyperparameter tuning. In order to improve your model's performance, you can include some other data augmentation methods. Feel free to play around and to implement other methods as for example Gaussian Blur or Rotation. </p>       \n",
    "</div>\n",
    "\n",
    "Let us quickly check the `RandomHorizontalFlip` method with an image of the Cifar10 dataset in the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 98
    },
    "executionInfo": {
     "elapsed": 962,
     "status": "ok",
     "timestamp": 1650011739465,
     "user": {
      "displayName": "Weber Anna",
      "userId": "18154618360144454414"
     },
     "user_tz": -120
    },
    "id": "3BEjmTGHN5eg",
    "outputId": "a6adee5b-1dd4-436c-e811-952f4ae12e5f"
   },
   "outputs": [],
   "source": [
    "#Load the data in a dataset without any transformation \n",
    "dataset = DATASET(\n",
    "        mode=mode,\n",
    "        root=cifar_root, \n",
    "        download_url=download_url,\n",
    "        split={'train': 0.6, 'val': 0.2, 'test': 0.2},\n",
    "    )\n",
    "\n",
    "#Retrieve an image from the dataset and flip it\n",
    "image = dataset[1]['image']\n",
    "transform = RandomHorizontalFlip(1)\n",
    "image_flipped = transform(image)\n",
    "\n",
    "#Show the two images\n",
    "plt.figure(figsize = (2,2))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(image.astype('uint8'))\n",
    "plt.axis('off')\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(image_flipped.astype('uint8'))\n",
    "plt.axis('off')\n",
    "plt.title(\"Left: Original Image, Right: Flipped image\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WKNRU4BWN5eg"
   },
   "source": [
    "## 1.3 Layers\n",
    "\n",
    "Now, that the data is prepared, we can take a look at the model we are using. In our case it will be a neural network. \n",
    "\n",
    "In Exercise 5, you implemented a simple 2-layer neural network that had a hidden size as a parameter:\n",
    "\n",
    "$$ \n",
    "{\\hat{y}} = \\sigma(\\sigma({x W_1} + {b_1}) {W_2} + {b_2}) \n",
    "$$\n",
    "\n",
    "where $ \\sigma({x}) $ was the sigmoid function, $ {x} $ was the input, $ {W_1}, {W_2} $ the weight matrices and $ {b_1}, {b_2}$ the biases for the two layers.\n",
    "\n",
    "This is how we used this network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6zta7tzkN5eg"
   },
   "outputs": [],
   "source": [
    "input_size = datasets['train'][0]['image'].shape[0]\n",
    "model = ClassificationNet(input_size=input_size, \n",
    "                          hidden_size=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UpAoXE7nN5eh"
   },
   "source": [
    "Note that we updated the ```ClassificationNet``` from the previous exercise. Now you can customize the number of outputs, the choice of activation function, the hidden size etc. We encourage you to check the implementation in ```exercise_code/networks/classification_net.py``` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7wzEaw66N5eh"
   },
   "outputs": [],
   "source": [
    "num_layer = 2\n",
    "reg = 0.1\n",
    "\n",
    "model = ClassificationNet(activation=Sigmoid(), \n",
    "                          num_layer=num_layer, \n",
    "                          reg=reg,\n",
    "                          num_classes=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x70gjgkDN5eh"
   },
   "source": [
    "Performing the forward and backward passes through the model was quite simple:\n",
    "\n",
    "```python\n",
    "\n",
    "# X is a batch of training features \n",
    "# X.shape = (batch_size, features_size)\n",
    "y_out = model.forward(X)\n",
    "\n",
    "# dout is the gradient of the loss function w.r.t the output of the network.\n",
    "# dout.shape = (batch_size, )\n",
    "model.backward(dout)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dlwNcT2RN5eh"
   },
   "source": [
    "Just as the learning rate or the number of epochs we want to train for, the number of hidden layers and the number of units in each hidden layer are also hyperparameters that we can tune. In this notebook you can play with networks of different sizes to see the impact that the network capacity has.\n",
    "\n",
    "Before we move on to the loss functions, let's have a look at the activation functions. The choice of an activation function can have a huge impact on the performance of the network you are designing. So far, you have implemented the `Sigmoid` and the `Relu` activation functions in Exercise 5. \n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "    <h3>Task: Check Code</h3>\n",
    "    <p>Take a look at the <code>Sigmoid</code> and the <code>Relu</code> class in <code>exercise_code/networks/layer.py</code> and the implementation of the respective forward and backward passes. Make sure to understand why we used <b>element-wise product</b> instead of dot product in the backward pass of the <code>Sigmoid</code> class to compute the gradient <code>dx</code>. That will be helpful for your later implementation of other activation functions.</p>\n",
    "    <p> <b>Note:</b> The <code>cache</code> variable is used to store information from the forward pass and then pass this information in the backward pass to make use of it there. The implementation of both classes shows that this variable can be used differently - depending on which information is needed in the backward pass. </p>\n",
    "</div>\n",
    "\n",
    "Now, we want to have a look at two other, very common activation functions that you have already seen in the lecture: Leaky ReLU activation function and Tanh activation function. \n",
    "\n",
    "**Leaky Relus** are one attempt to fix the “dying ReLU” problem. Instead of the function being zero when $ x < 0 $, a leaky ReLU has a small negative slope (for example, 0.01).  The function computes $f(x) = \\mathbb{1}(x < 0) (\\alpha x) + \\mathbb{1}(x>=0) (x)$ where $\\alpha$ is a small constant. Some people report success with this form of activation function, but the results are not always consistent.\n",
    "\n",
    "The **tanh non-linearity** squashes a real-valued number into the range [-1, 1]. Like the sigmoid neuron, its activations saturate, but unlike the sigmoid neuron its output is zero-centered. Therefore, in practice the tanh non-linearity is always preferred to the sigmoid non-linearity. Also note that the tanh neuron is simply a scaled sigmoid neuron, in particular the following holds: $\\tanh(x) = 2 \\cdot \\sigma(2x) -1$.\n",
    "\n",
    "<img class=left src=https://pytorch.org/docs/stable/_images/LeakyReLU.png alt=\"Figure3\" width=\"350\" align='left'/> \n",
    "<img class=right src=https://pytorch.org/docs/stable/_images/Tanh.png alt=\"Figure4\" width=\"350\"/>\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "    <h3>Task: Implement Activation Layers</h3>\n",
    "    <p> Now, it is your turn to implement the <code>LeakyRelu</code> and the <code>Tanh</code> class in <code>exercise_code/networks/layer.py</code> by completing the <code>forward</code> and the <code>backward</code> functions. You can test your implementation in the following two cells. </p>\n",
    "    <p> <b>Note:</b> Always remember to return a cache in <code>forward</code> for later backpropagation in <code>backward</code>. As we have seen above, the <code>cache</code> variable can be used differently for two activation functions.</p>\n",
    "</div>\n",
    "\n",
    "Use this cell to test your implementation of the `LeakyRelu` class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 854,
     "status": "ok",
     "timestamp": 1650011740634,
     "user": {
      "displayName": "Weber Anna",
      "userId": "18154618360144454414"
     },
     "user_tz": -120
    },
    "id": "v0LYvwHmN5ei",
    "outputId": "e124210f-f387-45b4-cd23-b0444c894b10"
   },
   "outputs": [],
   "source": [
    "from exercise_code.tests.layer_tests import *\n",
    "print(LeakyReluTestWrapper()())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kOkiYhe_N5ei"
   },
   "source": [
    "And this cell to test your implementation of the `Tanh` class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1650011740635,
     "user": {
      "displayName": "Weber Anna",
      "userId": "18154618360144454414"
     },
     "user_tz": -120
    },
    "id": "Do-AHImrN5ei",
    "outputId": "b4c5a743-2ae6-4700-e052-4c9c4a220d08"
   },
   "outputs": [],
   "source": [
    "print(TanhTestWrapper()())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mw_iJW4dN5ej"
   },
   "source": [
    "Congratulations, you implemented four different activation functions! These activation layers are now ready to be used when you start building your own network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3MhxqP2LN5ej"
   },
   "source": [
    "## 1.4 Loss\n",
    "\n",
    "In order to measure how well a network is performing, we implemented several ```Loss``` classes (```L1```, ```MSE```, ```BCE```, each preferred for a certain type of problems) in ```exercise_code/networks/loss.py```.\n",
    "\n",
    "Each contains a ```forward()``` method, which outputs a number we can use as a measure for our network's performance. \n",
    "\n",
    "Since our goal is to change the weights of the network in a way that this loss measure decreases, we are also interested in the gradients of the loss w.r.t the outputs of the network, $ \\nabla_{\\hat{y}} L({\\hat{y}}, {y}) $. This was implemented in ```backward()```. \n",
    "\n",
    "In previous exercises, we worked on binary classification problems and therefore used binary cross entropy (```BCE```) as a loss function.\n",
    "\n",
    "$$ BCE(\\hat{y}, y) = - \\frac{1}{N} \\sum_{i=1}^N \\Big [y_i \\log(\\hat{y_i}) + (1-y_i) \\log(1 - \\hat{y_i}) \\Big] $$ \n",
    "\n",
    "where\n",
    "- $ N $ was the number of samples we were considering\n",
    "- $\\hat{y}_i$ was the network's prediction for sample $i$. Note that this was a valid probability $\\in [0, 1]$, because we applied a [sigmoid](https://en.wikipedia.org/wiki/Sigmoid_function) activation on the last layer. \n",
    "- $ y_i $ was the ground truth label (0 or 1, depending on the class)\n",
    "\n",
    "Since we have 10 classes in the CIFAR10 dataset, we need a generalization of the binary cross entropy loss to multiple classes. This generalization is called the cross entropy loss and is defined as:\n",
    "$$ CE(\\hat{y}, y) = - \\frac{1}{N} \\sum_{i=1}^N \\sum_{k=1}^{C} \\Big[y_{ik} \\log(\\hat{y}_{ik}) \\Big] $$\n",
    "\n",
    "where:\n",
    "- $ N $ is the number of samples\n",
    "- $ C $ is the number of classes\n",
    "- $ \\hat{y}_{ik} $ is the probability that the model assigns for the $k$th class when the $i$th sample is the input. **Because we don't apply any activation function on the last layer of our network, its outputs for each sample will not be a valid probability distribution over the classes. We call these the raw outputs of the network '[logits](https://datascience.stackexchange.com/questions/31041/what-does-logits-in-machine-learning-mean/31045)' and we will apply a [softmax](https://en.wikipedia.org/wiki/Softmax_function) activation in order to obtain a valid probability distribution.** \n",
    "- $y_{ik} = 1 $ iff the true label of the $i$th sample is $k$ and 0 otherwise. This is called a [one-hot encoding](https://machinelearningmastery.com/why-one-hot-encode-data-in-machine-learning/).\n",
    "\n",
    "You can check for yourself that if the number of classes $ C $ is 2, the binary cross entropy is actually equivalent to the cross entropy.\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "    <h3>Task: Check Code</h3>\n",
    "    <p>Please check the implementation of the <code>CrossEntropyFromLogits</code> class, which can be found in <code>../exercise_06/exercise_code/networks/loss.py</code>. </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EBPuFF0BN5ej"
   },
   "outputs": [],
   "source": [
    "loss = CrossEntropyFromLogits()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y1HcQs25N5ek"
   },
   "source": [
    "We can simply get the results of the forward and backward passes as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jv2aujOvN5ek"
   },
   "source": [
    "```python\n",
    "# y_out is the output of the neural network\n",
    "# y_truth is the actual label from the dataset\n",
    "loss.forward(y_out, y_truth)\n",
    "loss.backward(y_out, y_truth)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dbFSP4z9N5ek"
   },
   "source": [
    "## 1.5 Optimizer\n",
    "\n",
    "Now that we know the gradient of the loss w.r.t the ouputs of the network, as well as the local gradient for each layer of the network, we can use the chain rule to compute all gradients. \n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "    <h3>Task: Check Code</h3>\n",
    "    <p>We implemented several optimizer classes <code>SGD</code>, <code>Adam</code>, <code>sgd_momentum</code> that use different first-order parameter update rules. Those can be found in <code>../exercise_06/exercise_code/networks/optimizer.py</code>. </p>\n",
    "    <p>The <code>step()</code> method used, iterates through all the parameters of the model and updates them using the gradient information.</p>\n",
    "</div>\n",
    "\n",
    "What the optimizer is doing, in pseudocode, is the following:\n",
    "\n",
    "```python\n",
    "for param in model:\n",
    "    # Use the gradient to update the weights.\n",
    "    update(param)\n",
    "    \n",
    "    # Reset the gradient after each update.\n",
    "    param.gradient = 0\n",
    "```\n",
    "\n",
    "```SGD``` had the simplest update rule:\n",
    "```python\n",
    "def update(param):\n",
    "    param = param - learning_rate * param.gradient\n",
    "```\n",
    "\n",
    "For the more complicated update rules, see ```exercise_code/networks/optimizer.py```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0TE2PxMBN5ek"
   },
   "source": [
    "## 1.6 Solver\n",
    "\n",
    "The ```Solver``` is where all the above elements come together: Given a train and a validation dataloader, a model, a loss and an optimizer, it uses the training data to optimize a model in order to get better predictions. We simply call ```train()``` and it does its 'magic' for us!\n",
    "```python\n",
    "solver = Solver(model, \n",
    "                dataloaders['train'], \n",
    "                dataloaders['val'], \n",
    "                learning_rate=0.001, \n",
    "                loss_func=MSE(), \n",
    "                optimizer=SGD)\n",
    "\n",
    "solver.train(epochs=epochs)\n",
    "```\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "    <h3>Task: Check Code</h3>\n",
    "    <p>Please check out the implementation of <code>train()</code> in <code>../exercise_06/exercise_code/solver.py</code>. </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QUFotyERN5ek"
   },
   "source": [
    "## 1.7 Weight Regularization\n",
    "\n",
    "Before finishing the recap, we want to take a look at some regularization methods that have been introduced in the lecture. Those can be helpful to improve the robustness of our model. In this chapter, we're talking about weight regularization methods.\n",
    "\n",
    "Weight regularization (a.k.a \"Weight Decay\") has been introduced to you as a method preventing the model from overfitting to the training data. \n",
    "\n",
    "The regularization term over the weights of the network is added to the loss:\n",
    "\n",
    "$$ L^* = \\underbrace{L}_{\\text{Overall loss}} + \\underbrace{\\lambda R(\\theta)}_{\\text{Regularization loss}} $$\n",
    "\n",
    "where $\\theta$ represents **ALL** the weights of the network, $\\theta = \\{W_1, \\dots, W_n\\}$ and $R(\\theta) \\in \\mathbb{R}.$\n",
    "\n",
    "Therefore, the backward step for each weight matrix $W_k \\in  \\theta$ is: \n",
    "\n",
    "$$\\frac{\\partial L}{\\partial W_k}^* = \\underbrace{\\frac{\\partial L}{\\partial W_k}}_{\\text{Gradient}}  + \\underbrace{\\lambda \\frac{\\partial R(\\theta)}{\\partial W_k}}_{\\text{Reg. loss gradient}}$$\n",
    "\n",
    "The usage of the regularization term encodes some preferences in manipulating the weights. In the lecture, we compared two weight regularization methods and their respective preference for weight vectors. We made the following observations: \n",
    "\n",
    "1. L1 regularization: Enforces sparsity \n",
    "2. L2 regularization: Enforces that weights have similar values\n",
    "\n",
    "The most common weight regularization method is the L2 regularization. The L2 regularization prefers smaller and more diffuse weight vectors. Therefore, the model is encouraged to take all input dimensions into account rather than focusing strongly on a small number of input dimensions.\n",
    "\n",
    "When using weight regularization, the loss function is a composition of two parts:\n",
    "\n",
    "The first being the data loss, which is calculated using Cross Entropy loss in our model. The second part is called the regularization loss $R(\\theta)$ and is computed in the L2 case as follows:\n",
    "$$R(\\theta) = \\sum_{k} \\sum_{i} \\sum_{j} w_{k,i,j}^2$$\n",
    "\n",
    "Where $k$ runs over all weight matrices in the network, and $i, j$ correspond to the spatial height and width of each weight matrix $W_k$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DxExi4hQN5el"
   },
   "source": [
    "# 2. An overview of hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "OdQde3JEN5el"
   },
   "source": [
    "\n",
    "<img src=https://images.deepai.org/glossary-terms/05c646fe1676490aa0b8cab0732a02b2/hyperparams.png alt=hyperparameter width=700>\n",
    "\n",
    "A **hyperparameter** is a parameter that is set before the learning process begins. Recall that the parameters of the weight matrix and the bias vector are learned during the learning process.\n",
    "\n",
    "The hyperparameter settings are essential, since they control and affect the whole training and therefore have a great impact on the model's performance. \n",
    "\n",
    "Some hyperparameters we have covered in lectures are:\n",
    "* Network architecture\n",
    "    * Choice of activation function\n",
    "    * Number of layers\n",
    "    * ...\n",
    "* Learning rate\n",
    "* Number of epochs\n",
    "* Batch size\n",
    "* Regularization strength\n",
    "* Momentum\n",
    "* ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rn6g8Zw1N5el"
   },
   "source": [
    "## 2.1 Start debugging your own network!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hUoj3OgzN5el"
   },
   "source": [
    "As already suggested in the lectures, you should always start from small and simple architectures, to make sure you are going the right way. \n",
    "\n",
    "As a first step you should try to overfit to a single training sample, then to a few batches of training samples and finally go deeper with larger neural networks and the whole training data.\n",
    "\n",
    "We provide a default neural network (i.e. ClassificationNet) with arbitrary number of layers, which is a generalization from a fixed 2-layer neural network in exercise 5. You are welcome to implement your own network, in that case just implement **MyOwnNetwork** in ```exercise_code/networks/classification_net.py```. You can also copy things from ClassficationNet and make little adjustments to your own network. Either way, just pick one network and comment out the other one, then run the cells below for debugging.\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "    <h3>Note:</h3>\n",
    "    <p>Please, make sure you don't modify the ClassificationNet itself. In this way you can always have a working network to fall back on.</p>\n",
    "    <p>In order to pass this submissions, you can <b>first stick to the default ClassificationNet implementation without changing any code at all</b>. The goal of this submission is to find reasonable hyperparameters and the parameter options of the ClassificationNet are broad enough to pass.</p>\n",
    "    <p>Once you have surpassed the submission goal, you can try to implement additional activation functions in the accompanying notebook, try different weight initializations or make other adjustments by writing your own network architecture in the MyOwnNetwork class.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gij9DxvmN5el"
   },
   "source": [
    "First, let's start with a 2-layer neural network, and overfit to one single training sample.\n",
    "\n",
    "After training, let's evaluate the training process by plotting the loss curves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 20902,
     "status": "ok",
     "timestamp": 1650011761531,
     "user": {
      "displayName": "Weber Anna",
      "userId": "18154618360144454414"
     },
     "user_tz": -120
    },
    "id": "S1k3JrlsN5el",
    "outputId": "1193ba97-9871-489f-d6a5-f21a3869f09c"
   },
   "outputs": [],
   "source": [
    "from exercise_code.solver import Solver\n",
    "from exercise_code.networks.optimizer import SGD, Adam\n",
    "from exercise_code.networks import MyOwnNetwork\n",
    "\n",
    "num_layer = 2\n",
    "epochs = 20\n",
    "reg = 0.1\n",
    "batch_size = 4\n",
    "\n",
    "model = ClassificationNet(num_layer=num_layer, reg=reg)\n",
    "# model = MyOwnNetwork()\n",
    "\n",
    "loss = CrossEntropyFromLogits()\n",
    "\n",
    "# Make a new data loader with a single training image\n",
    "overfit_dataset = DATASET(\n",
    "    mode='train',\n",
    "    root=cifar_root, \n",
    "    download_url=download_url,\n",
    "    transform=compose_transform,\n",
    "    limit_files=1\n",
    ")\n",
    "dataloaders['train_overfit_single_image'] = DataLoader(\n",
    "    dataset=overfit_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    drop_last=False,\n",
    ")\n",
    "\n",
    "# Decrease validation data for only debugging\n",
    "debugging_validation_dataset = DATASET(\n",
    "    mode='val',\n",
    "    root=cifar_root, \n",
    "    download_url=download_url,\n",
    "    transform=compose_transform,\n",
    "    limit_files=100\n",
    ")\n",
    "dataloaders['val_500files'] = DataLoader(\n",
    "    dataset=debugging_validation_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    ")\n",
    "\n",
    "solver = Solver(model, dataloaders['train_overfit_single_image'], dataloaders['val_500files'], \n",
    "                learning_rate=1e-3, loss_func=loss, optimizer=Adam)\n",
    "\n",
    "solver.train(epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 434,
     "status": "ok",
     "timestamp": 1650011761941,
     "user": {
      "displayName": "Weber Anna",
      "userId": "18154618360144454414"
     },
     "user_tz": -120
    },
    "id": "aMc51VqHN5em",
    "outputId": "730d9eb6-6b24-4411-e6b9-c165a73a7e6e"
   },
   "outputs": [],
   "source": [
    "plt.title('Loss curves')\n",
    "plt.plot(solver.train_loss_history, '-', label='train')\n",
    "plt.plot(solver.val_loss_history, '-', label='val')\n",
    "plt.legend(loc='lower right')\n",
    "plt.xlabel('Iteration')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 244,
     "status": "ok",
     "timestamp": 1650011762177,
     "user": {
      "displayName": "Weber Anna",
      "userId": "18154618360144454414"
     },
     "user_tz": -120
    },
    "id": "cchvV2Q_N5em",
    "outputId": "6ba579ba-58ed-43e8-f3e7-97a51018ee4b"
   },
   "outputs": [],
   "source": [
    "print(\"Training accuray: %.5f\" % (solver.get_dataset_accuracy(dataloaders['train_overfit_single_image'])))\n",
    "print(\"Validation accuray: %.5f\" % (solver.get_dataset_accuracy(dataloaders['val_500files'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HeTCu8wwN5em"
   },
   "source": [
    "This time we want to overfit to a small set of training batch samples. Please observe the difference from above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 32205,
     "status": "ok",
     "timestamp": 1650011794379,
     "user": {
      "displayName": "Weber Anna",
      "userId": "18154618360144454414"
     },
     "user_tz": -120
    },
    "id": "SsIu_JaUN5em",
    "outputId": "81810746-ea3e-4a60-f3b7-dad4230c7231"
   },
   "outputs": [],
   "source": [
    "from exercise_code.networks import MyOwnNetwork\n",
    "\n",
    "num_layer = 2\n",
    "epochs = 100\n",
    "reg = 0.1\n",
    "num_samples = 10\n",
    "\n",
    "model = ClassificationNet(num_layer=num_layer, reg=reg)\n",
    "# model = MyOwnNetwork()\n",
    "\n",
    "loss = CrossEntropyFromLogits()\n",
    "\n",
    "# Make a new data loader with a our num_samples training image\n",
    "overfit_dataset = DATASET(\n",
    "    mode='train',\n",
    "    root=cifar_root, \n",
    "    download_url=download_url,\n",
    "    transform=compose_transform,\n",
    "    limit_files=num_samples\n",
    ")\n",
    "dataloaders['train_overfit_10samples'] = DataLoader(\n",
    "    dataset=overfit_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    drop_last=False,\n",
    ")\n",
    "\n",
    "solver = Solver(model, dataloaders['train_overfit_10samples'], dataloaders['val_500files'], \n",
    "                learning_rate=1e-3, loss_func=loss, optimizer=Adam)\n",
    "\n",
    "solver.train(epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1650011794380,
     "user": {
      "displayName": "Weber Anna",
      "userId": "18154618360144454414"
     },
     "user_tz": -120
    },
    "id": "DmzTCYw_N5en",
    "outputId": "8076912f-608c-497c-e092-3da5e9e8ddcd"
   },
   "outputs": [],
   "source": [
    "plt.title('Loss curves')\n",
    "plt.plot(solver.train_loss_history, '-', label='train')\n",
    "plt.plot(solver.val_loss_history, '-', label='val')\n",
    "plt.legend(loc='lower right')\n",
    "plt.xlabel('Iteration')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 822,
     "status": "ok",
     "timestamp": 1650011795194,
     "user": {
      "displayName": "Weber Anna",
      "userId": "18154618360144454414"
     },
     "user_tz": -120
    },
    "id": "8H8xyMcaN5en",
    "outputId": "55c1d949-949d-414f-8a34-fa85faa9ed9b"
   },
   "outputs": [],
   "source": [
    "print(\"Training accuray: %.5f\" % (solver.get_dataset_accuracy(dataloaders['train_overfit_10samples'])))\n",
    "print(\"Validation accuray: %.5f\" % (solver.get_dataset_accuracy(dataloaders['val_500files'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7PoTJskAN5en"
   },
   "source": [
    "If you're overfitting to the training data, that means the network's implementation is correct. However, as you have more samples to overfit, your accuracy will be way lower. You can increase the number of epochs above to achieve better results.\n",
    "\n",
    "Now let's try to feed all the training and validation data into the network, but this time let's set compare a 2-layer and a 5-layer network, using the same hyperparameters.\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "    <h3>Note:</h3>\n",
    "    <p>This may take about 1 min per epoch as the training set is quite large. For convenience, we are now only using 1000 images for training but use the full validation set.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "ayhMGBUBN5en",
    "outputId": "0f77d4ee-a8c9-4ddc-e538-c1818c306acf"
   },
   "outputs": [],
   "source": [
    "from exercise_code.networks import MyOwnNetwork\n",
    "\n",
    "num_layer = 2\n",
    "epochs = 5\n",
    "reg = 0.01\n",
    "\n",
    "# Make a new data loader with 1000 training samples\n",
    "num_samples = 1000\n",
    "overfit_dataset = DATASET(\n",
    "    mode='train',\n",
    "    root=cifar_root, \n",
    "    download_url=download_url,\n",
    "    transform=compose_transform,\n",
    "    limit_files=num_samples\n",
    ")\n",
    "dataloaders['train_small'] = DataLoader(\n",
    "    dataset=overfit_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    drop_last=False,\n",
    ")\n",
    "\n",
    "\n",
    "# Change here if you want to use the full training set\n",
    "use_full_training_set = False\n",
    "if not use_full_training_set:\n",
    "    train_loader = dataloaders['train_small']\n",
    "else:\n",
    "    train_loader = dataloaders['train']\n",
    "    \n",
    "\n",
    "model = ClassificationNet(num_layer=num_layer, reg=reg)\n",
    "# model = MyOwnNetwork()\n",
    "\n",
    "loss = CrossEntropyFromLogits()\n",
    "\n",
    "solver = Solver(model, train_loader, dataloaders['val'], \n",
    "                learning_rate=1e-3, loss_func=loss, optimizer=Adam)\n",
    "\n",
    "solver.train(epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "3htGtgttN5en",
    "outputId": "6336d86c-13c1-40e5-9080-7aa5ed774959"
   },
   "outputs": [],
   "source": [
    "plt.title('Loss curves')\n",
    "plt.plot(solver.train_loss_history, '-', label='train')\n",
    "plt.plot(solver.val_loss_history, '-', label='val')\n",
    "plt.legend(loc='lower right')\n",
    "plt.xlabel('Iteration')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "oOVafQ7yN5eo",
    "outputId": "7207ecce-50aa-4e80-83ee-1d05529ab23a"
   },
   "outputs": [],
   "source": [
    "print(\"Training accuray: %.5f\" % (solver.get_dataset_accuracy(train_loader)))\n",
    "print(\"Validation accuray: %.5f\" % (solver.get_dataset_accuracy(dataloaders['val'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "vG9U5OmwN5eo",
    "outputId": "4aa8426b-eea9-4be8-bc08-6add66c4cf82"
   },
   "outputs": [],
   "source": [
    "from exercise_code.networks import MyOwnNetwork\n",
    "\n",
    "num_layer = 5\n",
    "epochs = 5\n",
    "reg = 0.01\n",
    "\n",
    "model = ClassificationNet(num_layer=num_layer, reg=reg)\n",
    "# model = MyOwnNetwork()\n",
    "\n",
    "# Change here if you want to use the full training set\n",
    "use_full_training_set = False\n",
    "if not use_full_training_set:\n",
    "    train_loader = dataloaders['train_small']\n",
    "else:\n",
    "    train_loader = dataloaders['train']\n",
    "\n",
    "loss = CrossEntropyFromLogits()\n",
    "\n",
    "solver = Solver(model, train_loader, dataloaders['val'], \n",
    "                learning_rate=1e-3, loss_func=loss, optimizer=Adam)\n",
    "\n",
    "solver.train(epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "08XZk2GYN5eo",
    "outputId": "3d04c7c9-a0ce-4522-c858-e9491fca8102"
   },
   "outputs": [],
   "source": [
    "plt.title('Loss curves')\n",
    "plt.plot(solver.train_loss_history, '-', label='train')\n",
    "plt.plot(solver.val_loss_history, '-', label='val')\n",
    "plt.legend(loc='lower right')\n",
    "plt.xlabel('Iteration')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "7-sI0yyWN5eo",
    "outputId": "29f489a2-6899-410e-9e82-8026cb77d505"
   },
   "outputs": [],
   "source": [
    "print(\"Training accuray: %.5f\" % (solver.get_dataset_accuracy(train_loader)))\n",
    "print(\"Validation accuray: %.5f\" % (solver.get_dataset_accuracy(dataloaders['val'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_nNdnSi4N5eo"
   },
   "source": [
    "As you can see from above, the same hyperparameter set can decrease the loss for a 2-layer network, but for 5-layer network, it hardly works.\n",
    "\n",
    "The steps above are already mentioned in the lectures as debugging steps before training a neural network. \n",
    "\n",
    "If you implement your own network, make sure you do the steps above before tuning the hyperparameters as below. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l0pZa4weN5eo"
   },
   "source": [
    "## 2.2 Difficulty in tuning hyperparameters\n",
    "Small decisions on hyperparameters count. Usually, but not always, hyperparameters cannot be learned using well known gradient based methods (such as gradient descent), which are commonly employed to learn parameters. \n",
    "\n",
    "As mentioned before, hyperparameters need to be set before training. Tuning hyperparameters is hard, because you always have to try different combinations of the hyperparameters, train the network, do the validation and pick the best one. Besides, it is not guaranteed that you'll find the best combination.\n",
    "\n",
    "Let's do some hands on learning using the hyperparameter tuning methods covered in the lectures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pDQl6fY6N5ep"
   },
   "source": [
    "# 3. Hyperparameter Tuning\n",
    "\n",
    "![alt text](https://blog.floydhub.com/content/images/2018/08/Screen-Shot-2018-08-22-at-17.59.25.png \"\")\n",
    "\n",
    "One of the main challenges in deep learning is finding the set of hyperparameters that performs best.\n",
    "\n",
    "So far, we have followed a manual approach by guessing hyperparameters, running the model, observing the result and maybe tweaking the hyperparameters based on this result. As you have probably noticed, this manual hyperparameter tuning is unstructured, inefficient and can become very tedious.\n",
    "\n",
    "\n",
    "A more systematic (and actually very simple) approach for hyperparameter tuning that you've already learned in the lecture  is implementing a **Grid Search**. \n",
    "\n",
    "\n",
    "\n",
    "## 3.1 Grid Search\n",
    "Grid search is a simple and naive, yet effective method to automate the hyperparameter tuning:\n",
    "\n",
    "* First, you define the set of parameters you want to tune, e.g. $\\{learning\\_rate, regularization\\_strength\\}$.\n",
    "\n",
    "* For each hyperparameter, you then define a set of possible values, e.g. $learning\\_rate = \\{0.0001, 0.001, 0.01, 0.1\\}$.\n",
    "\n",
    "* Then, you train a model for every possible combination of these hyperparameter values and afterwards select the combination that works best (e.g. in terms of accuracy on your validation set).\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "    <h3>Task: Check Code</h3>\n",
    "    <p>Check out our <code>grid_search</code> implementation in <code>../exercise_6/exercise_code/hyperparameter_tuning.py</code>. We show a simple for loop implementation and a more sophisticated one for multiple inputs. </p>\n",
    "</div>\n",
    " \n",
    " <div class=\"alert alert-warning\">\n",
    "    <h3>Note:</h3>\n",
    "    <p>To keep things simple in the beginning, it'll be enough to just focus on the hyperparameters <code>learning_rate</code> and <code>regularization_strength</code> here, as in the example above.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "BBW9Vl4rN5ep",
    "outputId": "39d57fd3-e250-4826-d0b6-89952b9ae73b"
   },
   "outputs": [],
   "source": [
    "from exercise_code.networks import MyOwnNetwork\n",
    "\n",
    "# Specify the used network\n",
    "model_class = ClassificationNet\n",
    "\n",
    "from exercise_code import hyperparameter_tuning\n",
    "best_model, best_config, results  = hyperparameter_tuning.grid_search(\n",
    "    dataloaders['train_small'], dataloaders['val_500files'],\n",
    "    grid_search_spaces = {\n",
    "        \"learning_rate\": [1e-2, 1e-3, 1e-4], \n",
    "        \"reg\": [1e-4]\n",
    "    },\n",
    "    model_class=model_class,\n",
    "    epochs=10, patience=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5v0g7q9xN5ep"
   },
   "source": [
    "From the results of your grid search, you might already have found some hyperparameter combinations that work better than others. A common practice is to now repeat the grid search on a more narrow domain centered around the parameters that worked best. \n",
    "\n",
    "**Conclusion Grid Search**\n",
    "\n",
    "With grid search we have automated the hyperparameter tuning to a certain degree. Another advantage is, that since the trainings of the models are independent of each other, you can parallelize the grid search, by e.g. trying out different hyperparameter configurations in parallel on different machines.\n",
    "\n",
    "However, as you have probably noticed, there is one big problem with this approach: the number of possible combinations grows exponentially with the number of hyperparameters (\"curse of dimensionality\"). As we add more hyperparameters to the grid search, the search space will explode in time complexity, making this strategy unfeasible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m_NvhoFcN5ep"
   },
   "source": [
    "Especially when your search space contains more than 3 or 4 dimensions, it is often better to use another, similar hyperparameter tuning method that you've already learned about: random search."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rDyeiAN1N5ep"
   },
   "source": [
    "## 3.2 Random Search\n",
    "Random search is very similar to grid search, with the only difference, that instead of providing specific values for every hyperparameter, you only define a range for each hyperparameter - then, the values are sampled randomly from the provided ranges.\n",
    "\n",
    "![alt text](https://i.stack.imgur.com/cIDuR.png \"\")\n",
    "\n",
    "The figure above illustrates the difference in the hyperparameter space exploration between grid search and random search: assume you have 2 hyperparameters with each 3 values. Running a grid search results in training $3^2=9$ different models - but in the end, you've just tired out 3 values for each parameter. For random search on the other hand, after training 9 models you'll have tried out 9 different values for each hyperparameter, which often leads much faster to good results.\n",
    "\n",
    "To get a deeper understanding of random search and why it is more efficient than grid search, you should definitely check out this paper: http://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "    <h3>Task: Check Code</h3>\n",
    "    <p>Check out our <code>random_search</code> implementation in <code>../exercise_6/exercise_code/hyperparameter_tuning.py</code></p>\n",
    "</div>\n",
    "\n",
    "\n",
    "*Hint: regarding the sample space of each parameter, think about the scale for which it makes most sense to sample in. For example the learning rate is usually sampled on a logarithmic scale!*\n",
    "\n",
    "*For simplicity and speed, just use the `train_small`-dataloader!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "d64soT68N5ep",
    "outputId": "eafe47b6-33db-401e-8d4e-96573caa8048"
   },
   "outputs": [],
   "source": [
    "from exercise_code.hyperparameter_tuning import random_search\n",
    "from exercise_code.networks import MyOwnNetwork\n",
    "\n",
    "# Specify the used network\n",
    "model_class = ClassificationNet\n",
    "\n",
    "best_model, best_config, results  = random_search(\n",
    "    dataloaders['train_small'], dataloaders['val_500files'],\n",
    "    random_search_spaces = {\n",
    "        \"learning_rate\": ([1e-2, 1e-6], 'log'),\n",
    "        \"reg\": ([1e-3, 1e-7], \"log\"),\n",
    "        \"loss_func\": ([CrossEntropyFromLogits()], \"item\")\n",
    "    },\n",
    "    model_class=model_class,\n",
    "    num_search = 1, epochs=20, patience=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LayF5YYbN5eq"
   },
   "source": [
    "It's time to run it with the whole dataset, and let it search for a few hours for a nice configuration. \n",
    "\n",
    "However, to save some time, let's first implement an **early-stopping** mechanism, that you also already know from the lecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DCaACnn3N5eq"
   },
   "source": [
    "## 3.3 Early Stopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IjsO1OPLN5eq"
   },
   "source": [
    "By now you've already seen a lot of training curves:\n",
    "\n",
    "<img src=http://fouryears.eu/wp-content/uploads/2017/12/early_stopping.png></img>\n",
    "\n",
    "Usually, at some point the validation loss goes up again, which is a sign that we're overfitting to our training data. Since it actually doesn't make sense to train further at this point, it's common practice to apply \"early stopping\", i.e., cancel the training process when the validation loss doesn't improve anymore. The nice thing about this concept is, that not only it improves generalization through the prevention of overfitting, but also it saves us a lot of time - one of our most valuable resources in deep learning.\n",
    "\n",
    "Since there are natural fluctuations in the validation loss, you usually don't cancel the training process right at the first epoch when the validation-loss increases, but instead, you wait for some epochs (specified by the `patience`-parameter) and if the loss still doesn't improve, we stop.\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "    <h3>Task: Check Code</h3>\n",
    "    <p>Please check the implementation of the early stopping mechanism in <code>../exercise_6/exercise_code/solver.py</code>.\n",
    " </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P8wWTovfN5eq"
   },
   "source": [
    "## 3.4 Let's find the perfect model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ksUYQAhgN5eq"
   },
   "source": [
    "Now you've set everything up and you are ready to train your model. You can use a combination of grid and random search to find proper hyperparameters. \n",
    "\n",
    "Be aware that this process will take some time, since we'll be using a much larger dataset.\n",
    "\n",
    "At the beginning, it is a good approach to do a coarse random search across a wide range of values to find promising sub-ranges of your parameter space. Afterwards, you can zoom into these ranges and perform another random search (or grid search) to finetune the configurations.\n",
    "\n",
    "To save time and resources, don't use the whole dataset at the beginning, but instead a medium large subset of the samples. Also, you don't have to train for a large number of epochs - as mentioned above: we first want to get an overview about our hyper parameters.\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "    <h3>Task: Hyperparameters Tunning & Model Training </h3>\n",
    "        <p> Now, it is your turn to do the hyperparamater tuning. In the cell below, you can use the <code>random_search</code> function to find a good choice of parameters. Put in some reasonable ranges for the hyperparameters and evaluate them.\n",
    "    <p> <b>Note:</b> At the beginning, it's a good approach to first do a coarse random search across a <b> wide range of values</b> to find promising sub-ranges of your parameter space and use <b> a medium large subset of the dataset </b>. Afterwards, you can zoom into these ranges and do another random search (or grid search) to finetune the configurations. Use the cell below to play around and find good hyperparameters for your model!</p>\n",
    "        <p> Finally, once you've found some promising hyperparameters (or narrowed them down to promising subranges), it's time to utilize these hyperparameters to train your network on the whole dataset for a large number of epochs so that your own model can reach an acceptable performance. \n",
    "        <p> <b>Hint 1:</b> You may use a <code>Solver</code> class we provided before or directly use the <code>random_search</code> function (as you can also monitor the loss here) for model training.\n",
    "        <p> <b>Hint 2:</b> Be patience, this will time.\n",
    "        <p> <b>Hint 3:</b> It is a better practice to find good set of hyperparameters on the small datasets, and only then run a full training session on the full dataset, either with specifc hyperparameters that you've found in the <code>random search</code>, or better ranges.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "g2C24gzzN5eq",
    "outputId": "f9a7ca76-0471-44bc-dbb7-ac7f5d14e581"
   },
   "outputs": [],
   "source": [
    "from exercise_code.networks import MyOwnNetwork\n",
    "\n",
    "best_model = ClassificationNet()\n",
    "#best_model = MyOwnNetwork()\n",
    "\n",
    "########################################################################\n",
    "# TODO:                                                                #\n",
    "# Implement your own neural network and find suitable hyperparameters  #\n",
    "# Be sure to edit the MyOwnNetwork class in the following code snippet #\n",
    "# to upload the correct model! Or just use the given                   #\n",
    "# \"ClassificationNet\".                                                 #\n",
    "#                                                                      #\n",
    "# Note: the pickling cell expects your model to be named \"best_model\". #\n",
    "# Unless you change it there, naming the best model in any other way   #\n",
    "# will result in an unknown behavior.                                  #\n",
    "########################################################################\n",
    "\n",
    "pass\n",
    "\n",
    "########################################################################\n",
    "#                           END OF YOUR CODE                           #\n",
    "########################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gmZbWaaIN5er"
   },
   "source": [
    "Now it's time to edit the ranges above and adjust them to explore regions that performed well!\n",
    "\n",
    "Also, feel free to experiment around. Other hyperparameters you can change are the network architecture, optimizer, activations functions and many more.\n",
    "\n",
    "Try to get an accuracy as high as possible, since that's all what counts for this submission!\n",
    "\n",
    "You'll pass if you reach at least **48%** accuracy on our test set - but there will also be a leaderboard of all students of this course. Can you make it to the top?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mJZ7UXHeN5er"
   },
   "source": [
    "## 3.5 Checking the validation accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "qPyHczNJN5er",
    "outputId": "7cbc0fbb-2d6e-49a1-d518-83e2260ea339"
   },
   "outputs": [],
   "source": [
    "labels, pred, acc = best_model.get_dataset_prediction(dataloaders['train'])\n",
    "print(\"Train Accuracy: {}%\".format(acc*100))\n",
    "labels, pred, acc = best_model.get_dataset_prediction(dataloaders['val'])\n",
    "print(\"Validation Accuracy: {}%\".format(acc*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MQ_AGtp8N5er"
   },
   "source": [
    "# 4. Test your model\n",
    "When you have finished the hyperparameter tuning and you found your final model which performs well on the validation set (**you should at least get 48% accuracy on the validation set!**), it's time to run your model on the test set.\n",
    "\n",
    "<div class=\"alert alert-danger\">\n",
    "    <h3>Important</h3>\n",
    "    <p>As you have learned in the lecture, you must only use the test set once! So only run the next cell if you are completely sure that your model works well enough and that you are ready to submit. Your test set is different from the test set on our server, so results may vary a bit. Nevertheless, you will get a reasonable approximation of your model's performance if you perform the final evaluation on the test set only once.</p>\n",
    "    <p>If you are an external student that can't use our submission webpage: this test performance is your final result and if you surpassed the threshold, you have completed this exercise :). Now, train again to aim for a better number!</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "xJ2lwiVUN5er",
    "outputId": "82f45e2a-7357-4444-d8c7-2296b7f6cb68"
   },
   "outputs": [],
   "source": [
    "# comment this part out to see your model's performance on the test set.\n",
    "labels, pred, acc = best_model.get_dataset_prediction(dataloaders['test'])\n",
    "print(\"Test Accuracy: {}%\".format(acc*100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K3MBnG7qN5er"
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <h3>Note:</h3>\n",
    "    <p>The \"real\" test set is actually the dataset we're using for testing your model, which is <b>different</b> from the test-set you're using here.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2lOy_GulN5es"
   },
   "source": [
    "# 5. Saving your Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "icgNMAlEN5es"
   },
   "outputs": [],
   "source": [
    "from exercise_code.tests import save_pickle\n",
    "save_pickle({\"cifar_fcn\": best_model}, \"cifar_fcn.p\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "Gd4TchGlN5es",
    "outputId": "b913c1bd-b861-47bb-c257-b30fa214e761"
   },
   "outputs": [],
   "source": [
    "from exercise_code.submit import submit_exercise\n",
    "\n",
    "submit_exercise('../output/exercise_06')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q5C_bKa5N5es"
   },
   "source": [
    "# 6. Submission Instructions\n",
    "\n",
    "Congratulations! You've just built your first image classifier! To complete the exercise, submit your final model to our submission portal - you probably know the procedure by now.\n",
    "\n",
    "1. Go on [our submission page](https://i2dl.vc.in.tum.de/submission/), register for an account and login. We use your matriculation number and send you an email with the login details to the associated mail account. When in doubt, login into tum-online and check your mails there. You will get an id which we'll need in the next step.\n",
    "2. Log into [our submission page](https://i2dl.vc.in.tum.de/submission/), with your account details and upload the zip file.\n",
    "3. Your submission will be evaluated by our system and you will get feedback about the performance of it. You will get an email with your score, as well as a message if you have surpassed the threshold or not.\n",
    "4. Within the working period, you can submit as many solutions as you want to get the best possible score.\n",
    "\n",
    "\n",
    "# 7. Submission Goals\n",
    "\n",
    "- Goal: Successfully implement a fully connected NN image classifier and tune the hyperparameters.\n",
    "\n",
    "- Passing Criteria: This time, there are no unit tests checking specific components of your code. To  pass the submission your model needs to reach at least **48% accuracy** on __our__ test dataset. The submission system will show you a number between 0 and 100 which corresponds to your accuracy.\n",
    "\n",
    "- Submission start: __December 1st, 2022 13:00__\n",
    "- Submission deadline : __December 7th, 2022 15:59__ \n",
    "- You can make **$\\infty$** submissions until the deadline. Your __best submission__ will be considered for the bonus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Exercise Review](https://forms.gle/9SYivCPQZdktRDS29)\n",
    "We are always interested in your opinion. Now that you have finished this exercise, we would like you to give us some feedback about the time required to finish the submission and/or work through the notebooks. Please take the short time to fill out our [review form](https://forms.gle/9SYivCPQZdktRDS29) for this exercise so that we can do better next time! :)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "WKNRU4BWN5eg",
    "3MhxqP2LN5ej",
    "dbFSP4z9N5ek",
    "0TE2PxMBN5ek",
    "QUFotyERN5ek",
    "l0pZa4weN5eo",
    "rDyeiAN1N5ep",
    "DCaACnn3N5eq",
    "P8wWTovfN5eq",
    "mJZ7UXHeN5er"
   ],
   "name": "1.cifar10_classification.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.8.15 ('AD')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "0bf50f5aabac69db0919c0488dbd4d75a157e00f1ef8368b4ea3c163bd36e766"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
