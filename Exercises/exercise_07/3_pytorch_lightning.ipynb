{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch Lightning Introduction\n",
    "\n",
    "Welcome to the introduction to [`PyTorchLightning`](https://www.pytorchlightning.ai/). PyTorch Lightning is a wrapper for PyTorch that is focused towards building neural networks model quickly by removing the boilerplate code. It also extends the functionality of PyTorch, for example, with model Callbacks and automatic porting to GPU to accelerate computations.\n",
    "\n",
    "In its essence, pytorch lightning provides a controlled setup where you are forced to in its specific partly automated library setup, but are able to customize/de-automatate each process if you so desire. You can both rely on a variety of already implemented functionalities and the fact that your code is by definition structured clearly, but at the cost of having to delve into their code structure and figuring out which functions to use.\n",
    "\n",
    "Our suggestion is to try it out. If you like it, cool, otherwise you can just stick to pytorch and build up your own library of functions you will use throughout your Deep Learning journey.\n",
    "\n",
    "Let's get started by installing PyTorch Lightning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Optional) Mount folder in Colab\n",
    "\n",
    "Uncomment thefollowing cell to mount your gdrive if you are using the notebook in google colab:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "from google.colab import drive\n",
    "import os\n",
    "\n",
    "gdrive_path='/content/gdrive/MyDrive/i2dl/exercise_07'\n",
    "\n",
    "# This will mount your google drive under 'MyDrive'\n",
    "drive.mount('/content/gdrive', force_remount=True)\n",
    "# In order to access the files in this notebook we have to navigate to the correct folder\n",
    "os.chdir(gdrive_path)\n",
    "# Check manually if all files are present\n",
    "print(sorted(os.listdir()))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: install correct libraries in google colab\n",
    "# !python -m pip install torch==1.11.0+cu113 torchvision==0.12.0+cu113 -f https://download.pytorch.org/whl/torch_stable.html\n",
    "# !python -m pip install torchtext==0.12.0 torchaudio==0.11.0\n",
    "# !python -m pip install tensorboard==2.9.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installing Pytorch Lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For automatic file reloading as usual\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "# For google colab\n",
    "# !python -m pip install pytorch-lightning==1.6.0 > /dev/null\n",
    "\n",
    "# For anaconda/regular python\n",
    "# !{sys.executable} -m pip install pytorch-lightning==1.6.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "print(f\"Lightning version: {pl.__version__}\")\n",
    "if not pl.__version__.startswith(\"1.6\"):\n",
    "    print(\"You are using another version of pytorch lightning. We expect pytorch lightning 1.6.0. You can continue with your version but it\"\n",
    "          \" might cause dependency and compatibility issues.\")\n",
    "\n",
    "import tensorboard\n",
    "import torch\n",
    "import torchvision\n",
    "print(f\"Tensorboard version: {tensorboard.__version__}\")\n",
    "if not tensorboard.__version__.startswith(\"2.9.1\"):\n",
    "    print(\"You are using an another version of Tensorboard. We expect Tensorboard 2.9.1 You may continue using your version but it\"\n",
    "          \" might cause dependency and compatibility issues.\")\n",
    "print(f\"PyTorch version Installed: {torch.__version__}\\nTorchvision version Installed: {torchvision.__version__}\\n\")\n",
    "if not torch.__version__.startswith(\"1.11\"):\n",
    "    print(\"you are using an another version of PyTorch. We expect PyTorch 1.11.0. You may continue using your version but it\"\n",
    "          \" might cause dependency and compatibility issues.\")\n",
    "if not torchvision.__version__.startswith(\"0.12\"):\n",
    "    print(\"you are using an another version of torchvision. We expect torchvision 0.12. You can continue with your version but it\"\n",
    "          \" might cause dependency and compatibility issues.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Idea behind PyTorch Lightning\n",
    "\n",
    "Codes in a Deep learning project consists of three main categories:\n",
    "\n",
    "1. **Research code**   \n",
    "    This is the exciting part of the experiment where you configure the model architecture and try out different optimizers and target task. This is managed by the `LightningModule` of PyTorch Lightning.\n",
    "    \n",
    "2. **Engineering code**  \n",
    "    This is the same set of code that remain the same for all deep learning projects.Recall the training block of previous notebooks where we loop through the epochs and mini-batches. The `Trainer` class of PyTorch Lightning takes care of this part of code.\n",
    "    \n",
    "3. **Non-essential code**\n",
    "    It is very important that we log our training metrics and organize different training runs to have purposeful experimentation of models. The `Callbacks` class PyTorch Lightning helps us with this section. \n",
    "\n",
    "Let's look at each of these modules in detail.\n",
    "\n",
    "1. **LightningModules** contain all model related code. This is the part where we are working on when creating a new project. The idea is to have all important code in one module, e.g., the model's architecture and the evaluation of training and validation metrics. This provides a better overview as repeated elements, such as the training procedure, are not stored in the code that we work on. The lightning module also handles the calls `.to(device)` or `.train()` and `.eval()`. Hence, there is no need anymore to switch between the cpu and gpu and to take care of the model's mode as this is automated by the LightningModule. The framework also enables easy parallel computation on multiple gpus. \n",
    "\n",
    "2. **Trainer** contains all code needed for training our neural networks that doesn't change for each project (\"one size fits all\"). Usually, we don't touch the code automated by this class. The arguments that are specific for one training such as learning rate and batch size are provided as initialization arguments for the LightningModule.\n",
    "\n",
    "3. **Callbacks** automate all parts needed for logging hyperparameters or training results such as the tensorboard logger. Logging becomes very important for research later since the results of experiments need to be reproducible.\n",
    "\n",
    "All in all, PyTorch is a framework that handles all (annoying) \"engineering\" stuff for you such that you have more time for exciting research and scientific coding. This also results in the advantage that automated parts are guaranteed to be bug-free. Hence, you can't include a bug in a part of your code that is often used but not often checked. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Overview of the PyTorch Lightning code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Research relevant code goes into the `LightningModule`. The advantage is that we have all the model building, training & validation steps within a single class. These are the components that usually change based on the projects and tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![tensorBoard Interface](./images/pl_quick_start_full_compressed.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The remaining code is automated by the `Trainer` class which takes care of the tasks of our mechanical training loops components such as iterating through the minibatches and gradient updating steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](https://miro.medium.com/max/700/1*b81_j__xv8M0Bb6nFTXbAA.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could already see how much more readable and concise our code is, after being transformed by PyTorch Lightning.\n",
    "\n",
    "Let us now train a neural network model with PyTorch Lightning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Training with PyTorch Lightning\n",
    "\n",
    "We will build a two-layer neural network to train on the the [`Fashion-MNIST`](https://research.zalando.com/welcome/mission/research-projects/fashion-mnist/) dataset for this notebook. \n",
    "\n",
    "## 3.1 Define A LightningModule\n",
    "\n",
    "We define our network as an instance of `pl.LightningModule` which replaces our `PyTorch` network based on the class `nn.Module`. Additionally, it contains all the relevant parts that are used for training and evaluating different models on various tasks.  \n",
    "\n",
    "Let's have a look at the implementation of `TwoLayerNet` in `exercise_code.lightning_models`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `__init__()` and `forward()` function defining the forward  pass remain the same. Hence, we can just copy the code from the `nn.Module`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "class TwoLayerNet(pl.LightningModule):\n",
    "    def __init__(self, hparams, input_size=1 * 28 * 28, hidden_size=512, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters(hparams)\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(hidden_size, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # flatten the image  before sending as input to the model\n",
    "        N, _, _, _ = x.shape\n",
    "        x = x.view(N, -1)\n",
    "\n",
    "        x = self.model(x)\n",
    "\n",
    "        return x\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now define the training  and validation steps since they also vary with different tasks and projects. Consequently, it is useful to integrate these parts into our instance of `LightningModule`. Validation loss is returned for each validation mini-batch and averaged at the end of the epoch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        images, targets = batch\n",
    "\n",
    "        # Perform a forward pass on the network with inputs\n",
    "        out = self.forward(images)\n",
    "\n",
    "        # calculate the loss with the network predictions and ground truth targets\n",
    "        loss = F.cross_entropy(out, targets)\n",
    "\n",
    "        # Find the predicted class from probabilities of the image belonging to each of the classes\n",
    "        # from the network output\n",
    "        _, preds = torch.max(out, 1)\n",
    "\n",
    "        # Calculate the accuracy of predictions\n",
    "        acc = preds.eq(targets).sum().float() / targets.size(0)\n",
    "\n",
    "        # Log the accuracy and loss values to the tensorboard\n",
    "        self.log('loss', loss)\n",
    "        self.log('acc', acc)\n",
    "\n",
    "        return {'loss': loss}\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        images, targets = batch\n",
    "\n",
    "        # Perform a forward pass on the network with inputs\n",
    "        out = self.forward(images)\n",
    "\n",
    "        # calculate the loss with the network predictions and ground truth targets\n",
    "        loss = F.cross_entropy(out, targets)\n",
    "\n",
    "        # Find the predicted class from probabilities of the image belonging to each of the classes\n",
    "        # from the network output\n",
    "        _, preds = torch.max(out, 1)\n",
    "\n",
    "        # Calculate the accuracy of predictions\n",
    "        acc = preds.eq(targets).sum().float() / targets.size(0)\n",
    "\n",
    "        # Visualise the predictions  of the model\n",
    "        if batch_idx == 0:\n",
    "            self.visualize_predictions(images, out.detach(), targets)\n",
    "\n",
    "        return {'val_loss': loss, 'val_acc': acc}\n",
    "\n",
    "    def validation_epoch_end(self, outputs):\n",
    "\n",
    "        # Average the loss over the entire validation data from it's mini-batches\n",
    "        avg_loss = torch.stack([x['val_loss'] for x in outputs]).mean()\n",
    "        avg_acc = torch.stack([x['val_acc'] for x in outputs]).mean()\n",
    "\n",
    "        # Log the validation accuracy and loss values to the tensorboard\n",
    "        self.log('val_loss', avg_loss)\n",
    "        self.log('val_acc', avg_acc)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last step missing in our `LightningModule` is the optimizer. This method needs to be defined in every `LightningModule`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "    def configure_optimizers(self):\n",
    "        optim = torch.optim.SGD(self.model.parameters(), self.hparams[\"learning_rate\"], momentum=0.9)\n",
    "\n",
    "        return optim\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have set up the model and the training steps, we will now establish the data pipeline. PyTorch Lightning provides the `LightningDataModule` for setting up the dataloaders.\n",
    "\n",
    "Let's have a look at the implementation of `FashionMNISTDataModule` in `exercise_code.data_class`.\n",
    "\n",
    "The `prepare_data()` function intends to set up the dataset and the related transforms for it. As previously, we download the `FashionMNIST` dataset using `torchvision` and split the total training data into a training and validation set for tuning hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "class FashionMNISTDataModule(pl.LightningDataModule):\n",
    "\n",
    "    def __init__(self, batch_size=4):\n",
    "        super().__init__()\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def prepare_data(self):\n",
    "\n",
    "        # Define the transform\n",
    "        transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                        transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "        # Download the Fashion-MNIST dataset\n",
    "        fashion_mnist_train_val = torchvision.datasets.FashionMNIST(root='../datasets', train=True,\n",
    "                                                                   download=True, transform=transform)\n",
    "\n",
    "        self.fashion_mnist_test = torchvision.datasets.FashionMNIST(root='../datasets', train=False,\n",
    "                                                                 download=True, transform=transform)\n",
    "\n",
    "        # Apply the Transforms\n",
    "        transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                        transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "        # Perform the training and validation split\n",
    "        self.train_dataset, self.val_dataset = random_split(\n",
    "            fashion_mnist_train_val, [50000, 10000])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We shall now define `Dataloaders` for each of the data-splits. These data loaders can be directly called during model training!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset, batch_size=self.batch_size)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_dataset, batch_size=self.batch_size)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.fashion_mnist_test, batch_size=self.batch_size)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can notice now that most of the code of these steps can be directly copied from a Vanilla PyTorch code. Lightning just rearranges them. This marks the end of the research part of the code.\n",
    "\n",
    "Let's see now how the `Trainer` class works:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  3.2 Fitting the model with a Trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will initialize the model and the data  with a set of hyperparameters given in the dictionary `hparams`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output \n",
    "\n",
    "from exercise_code.lightning_models import TwoLayerNet\n",
    "from exercise_code.data_class import FashionMNISTDataModule\n",
    "\n",
    "hparams = {\n",
    "    \"batch_size\": 16,\n",
    "    \"learning_rate\": 1e-3,\n",
    "    \"input_size\": 1 * 28 * 28,\n",
    "    \"hidden_size\": 512,\n",
    "    \"num_classes\": 10,\n",
    "    \"num_workers\": 2,    # used by the dataloader, more workers means faster data preparation, but for us this is not a bottleneck here\n",
    "}\n",
    "\n",
    "\n",
    "model = TwoLayerNet(hparams)\n",
    "data=FashionMNISTDataModule(hparams)\n",
    "data.prepare_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " PyTorch Lightning provides ample flexibility for training using [`Trainer`](https://pytorch-lightning.readthedocs.io/en/latest/trainer.html) class.\n",
    "Have a look at the documentation to know more about them!\n",
    "\n",
    "Let's initialize it now!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = pl.Trainer(\n",
    "    max_epochs=2,\n",
    "    accelerator=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The argument `max_epochs` sets the maximum number of epochs for training. \n",
    "The argument `weights_summary` prints a summary of the number of weights per layer at the beginning of the training. Set it to None if the summary is not required.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here comes the actual training cell. The [`fit`](https://pytorch-lightning.readthedocs.io/en/latest/_modules/pytorch_lightning/trainer/trainer.html#Trainer.fit) function takes in the model and data to train the model with a lot more optional arguments for customization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit(model, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checkout the directory `lightning_logs`. For each run there is a new directory `version_xx` created. The rightmost argument in the progress bar, the `v_num` variable above shows the version of the current run. Each directory automatically contains a folder with checkpoints, logs and the hyperparameters for this run.\n",
    "\n",
    "As seen in the last notebook, you can have a look at the  logs of the runs in the TensorBoard  \n",
    "Use the command as in the previous notebook in your terminal \n",
    "```\n",
    "tensorboard --logdir lightning_logs\n",
    "```\n",
    "Make sure to use the above command as the same directory as `exercise_07`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are using Google Colab, run the following cell to load the TensorBoard extension within the notebook. You may have to scroll to this block whenever you need to look at the TensorBoard interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext tensorboard\n",
    "# %tensorboard --logdir lightning_logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Add images to tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tensorboard logger is a submodule of the `LightningModule` and can be accessed via `self.logger`. We can  add images  to the logging module by calling \n",
    "```python\n",
    "self.logger.experiment.add_image('tag', image)\n",
    "```\n",
    "to add an image. \n",
    "\n",
    "\n",
    "We will log the first batch of validation images in a grid together with the predicted class labels and the ground truth labels. \n",
    "\n",
    "```python\n",
    "        if batch_idx == 0:\n",
    "            self.visualize_predictions(images, out.detach(), targets)\n",
    "```\n",
    "\n",
    "Let's have a look at the implementation of `visualize_predictions()` function in `exercise_code.lightning_models`.\n",
    "\n",
    "\n",
    "\n",
    "```python\n",
    "    def visualize_predictions(self, images, preds, targets):\n",
    "        class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', \n",
    "                       'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "\n",
    "        # determine size of the grid based on given batch size\n",
    "        num_rows = torch.tensor(len(images)).float().sqrt().floor()\n",
    "        \n",
    "        fig = plt.figure(figsize=(10, 10))\n",
    "        for i in range(len(images)):\n",
    "            plt.subplot(num_rows ,len(images) // num_rows + 1, i+1)\n",
    "            plt.imshow(images[i].permute(1, 2, 0))\n",
    "            plt.title(class_names[torch.argmax(preds, axis=-1)[i]] + f'\\n[{class_names[targets[i]]}]')\n",
    "            plt.axis('off')\n",
    "\n",
    "        self.logger.experiment.add_figure('predictions', fig, global_step=self.global_step)\n",
    "```\n",
    "\n",
    "You can view the logged images in your `IMAGES` tab of TensorBoard."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have now looked at how to train a model using PyTorch Lightning. PyTorch Lightning is very active in developement and the features set are continously expanded and updated.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Other Features of PyTorch Lightning\n",
    "\n",
    "\n",
    "### Checking  training timings\n",
    "\n",
    "The argument `profiler` of the `Trainer` class measures the time taken in different steps such as dataloading, forward and backward pass. You can select a variety of loggers [here](https://pytorch-lightning.readthedocs.io/en/stable/advanced/profiler.html).\n",
    "\n",
    "Run the cell below to see for yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = pl.Trainer(\n",
    "    profiler='simple',\n",
    "    max_epochs=1,\n",
    "    accelerator=\"auto\"\n",
    ")\n",
    "\n",
    "trainer.fit(model, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see an overview of the time taken for different steps.\n",
    "This  enables us to detect bottlenecks in the model more easily. A bottleneck can be, for example, long times in dataloading. It becomes very important later, especially, when you start to implement custom layers or loss functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some more debugging Options\n",
    "\n",
    "* [`fast_dev_run`](https://pytorch-lightning.readthedocs.io/en/latest/trainer.html#fast-dev-run): Runs of batch of each train, validation and test pass (if validation and test datalaoders are passed as arguments).This is a fast way to check if everything works (dataloading, validation metric, model saving/ loading) without having to wait for a full epoch.\n",
    "\n",
    "* [`track_grad_norm`](https://pytorch-lightning.readthedocs.io/en/latest/trainer.html#track-grad-norm): Logs the  norm of the gradients (set to `1` for the $L1$ norm or `2` for the $L2$ norm) for each layer. You can check whether the network is actually doing something. If the gradients are too small or too high, you won't have a good training (due to vanishing/ exploding gradients).\n",
    "\n",
    "\n",
    "### Other Features\n",
    "\n",
    "Finally, we want to mention some other useful options in the Trainer class:\n",
    "\n",
    "* [`resume_from_checkpoint`](https://pytorch-lightning.readthedocs.io/en/latest/trainer.html#resume-from-checkpoint): Start the training from a checkpoint saved earlier. Argument is the path to the saved model file.\n",
    "* [`Callbacks`](https://pytorch-lightning.readthedocs.io/en/latest/callbacks.html#callback): Callbacks are extremely useful system during training that automate non essential code such as  storing model checkpoints , saving weights values among others.\n",
    "\n",
    "Let's have the look at the [`EarlyStopping`](https://pytorch-lightning.readthedocs.io/en/latest/early_stopping.html#early-stopping-based-on-metric-using-the-earlystopping-callback)  callback.\n",
    "\n",
    "It interrupts the training if the `monitor` metric variable does not  improve for `patience` number of epochs.\n",
    "\n",
    "Below is a code example on how to apply it!\n",
    "\n",
    "```python\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "\n",
    "early_stop_callback = EarlyStopping(\n",
    "   monitor='val_accuracy',\n",
    "   patience=3,\n",
    "   verbose=False,\n",
    "   mode='max'\n",
    ")\n",
    "\n",
    "trainer = Trainer(max_epochs=10,callbacks=[early_stop_callback])\n",
    "```\n",
    "\n",
    "Your journey with pytorch lightning will force you to check out a bunch of callbacks to return the desired functions without having to write the respective code yourself. Or you hack it in, do as you wish :)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "1. PyTorch Lightining [`Source Code`](https://github.com/PyTorchLightning/pytorch-lightning) with a nice introduction \n",
    "2. PyTorch Lightining [`Documentation`](https://pytorch-lightning.readthedocs.io/en/latest/#)  Explore it! The features are very well explained. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "i2dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "322ad45fa68ebe5c4128075a6824acd1cfa71405c5d47c425d606c0b05091c60"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
