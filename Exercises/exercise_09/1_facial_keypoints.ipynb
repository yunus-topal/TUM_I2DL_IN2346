{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jvuNZQOgCPTc"
   },
   "source": [
    "# Facial Keypoint Detection\n",
    "\n",
    "Welcome back for exercise 9! As we told you, the exercises of this lecture can be subdivided into mainly two parts. The first part in which we re-invented the wheel and implemented the most important methods on our own and the second part, where we start using existing libraries (that already have implemented all the methods). It's about time to start playing around with more complex network architectures. \n",
    "\n",
    "We've already entered stage two, but with the introduction of convolution neural networks this week, we are given a very powerful tool that we want to explore in this exercises. Therefore, in this week's exercise your task is to build a convolution neural network to perform facial keypoint detection. \n",
    "\n",
    "Before we start, let's take a look at some example images and corresponding facial keypoints:\n",
    "\n",
    "<img src='images/key_pts_example.png' width=70% height=70%/>\n",
    "\n",
    "The facial keypoints (also called facial landmarks) are the small magenta dots shown on each of the faces in the images above. These keypoints mark important areas of the face: the eyes, corners of the mouth, the nose, etc. and are relevant for a variety of computer vision tasks, such as face filters, emotion recognition, pose recognition, and more. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PYvPUcpPCPTh"
   },
   "source": [
    "## (Optional) Mount folder in Colab\n",
    "\n",
    "Uncomment thefollowing cell to mount your gdrive if you are using the notebook in google colab:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 27848,
     "status": "ok",
     "timestamp": 1656687919453,
     "user": {
      "displayName": "Dan Halperin",
      "userId": "04461491770918170797"
     },
     "user_tz": -120
    },
    "id": "XThLiWNqCPTi",
    "outputId": "b75330d6-9590-43d7-e46b-bf5919e4ca97"
   },
   "outputs": [],
   "source": [
    "# Use the following lines if you want to use Google Colab\n",
    "# We presume you created a folder \"i2dl\" within your main drive folder, and put the exercise there.\n",
    "# NOTE: terminate all other colab sessions that use GPU!\n",
    "# NOTE 2: Make sure the correct exercise folder (e.g exercise_09) is given.\n",
    "\n",
    "\"\"\"\n",
    "from google.colab import drive\n",
    "import os\n",
    "\n",
    "gdrive_path='/content/gdrive/MyDrive/i2dl/exercise_09'\n",
    "\n",
    "# This will mount your google drive under 'MyDrive'\n",
    "drive.mount('/content/gdrive', force_remount=True)\n",
    "# In order to access the files in this notebook we have to navigate to the correct folder\n",
    "os.chdir(gdrive_path)\n",
    "# Check manually if all files are present\n",
    "print(sorted(os.listdir()))\n",
    "\n",
    "# Relevant packages. NOTE: Be patient, it might take a few minutes.\n",
    "!python -m pip install tensorboard==2.9.1 pytorch-lightning==1.6.0 torchtext\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7dQywr2PCPTj"
   },
   "source": [
    "# 1. Preparation\n",
    "\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 10287,
     "status": "ok",
     "timestamp": 1656687929737,
     "user": {
      "displayName": "Dan Halperin",
      "userId": "04461491770918170797"
     },
     "user_tz": -120
    },
    "id": "gFK8pPQpCPTk"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from exercise_code.data.facial_keypoints_dataset import FacialKeypointsDataset\n",
    "from exercise_code.networks.keypoint_nn import (\n",
    "    DummyKeypointModel,\n",
    "    KeypointModel\n",
    ")\n",
    "from exercise_code.util import (\n",
    "    show_all_keypoints,\n",
    "    save_model,\n",
    ")\n",
    "from exercise_code.tests import test_keypoint_nn\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PzzrR3TuCPTk"
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <h3>Note: Google Colab</h3>\n",
    "    <p>\n",
    "In case you don't have a GPU, you can run this notebook on Google Colab where you can access a GPU for free, but you can also run this notebook on your CPU.\n",
    "         </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 352,
     "status": "ok",
     "timestamp": 1656687930078,
     "user": {
      "displayName": "Dan Halperin",
      "userId": "04461491770918170797"
     },
     "user_tz": -120
    },
    "id": "4IeJlGMsCPTl",
    "outputId": "414917b6-6003-4960-c6c5-4e633b5ce420"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m6clRitvCPTm"
   },
   "source": [
    "## Load and Visualize Data\n",
    "To load the data, we have already prepared a Pytorch Dataset class `FacialKeypointsDataset` for you. You can find it in `exercise_code/data/facial_keypoints_dataset.py`. Run the following cell to download the data and initialize your dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7291,
     "status": "ok",
     "timestamp": 1656687937366,
     "user": {
      "displayName": "Dan Halperin",
      "userId": "04461491770918170797"
     },
     "user_tz": -120
    },
    "id": "JURilyFQCPTm",
    "outputId": "7d1aa7c9-174c-4bc2-a785-4421658e35b2"
   },
   "outputs": [],
   "source": [
    "download_url = \"https://vision.in.tum.de/webshare/g/i2dl/facial_keypoints.zip\"\n",
    "i2dl_exercises_path = os.path.dirname(os.path.abspath(os.getcwd()))\n",
    "data_root = os.path.join(i2dl_exercises_path, \"datasets\", \"facial_keypoints\")\n",
    "train_dataset = FacialKeypointsDataset(\n",
    "    train=True,\n",
    "    transform=transforms.ToTensor(),\n",
    "    root=data_root,\n",
    "    download_url=download_url,\n",
    ")\n",
    "val_dataset = FacialKeypointsDataset(\n",
    "    train=False,\n",
    "    transform=transforms.ToTensor(),\n",
    "    root=data_root,\n",
    ")\n",
    "\n",
    "print(\"Number of training samples:\", len(train_dataset))\n",
    "print(\"Number of validation samples:\", len(val_dataset))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zHlKmzEjCPTn"
   },
   "source": [
    "Each sample in our dataset is a dict `{\"image\": image, \"keypoints\": keypoints}`, where\n",
    " * `image` is a [0-1]-normalized gray-scale image of size 96x96, represented by a torch tensor of shape (CxHxW) with C=1, H=96, W=96\n",
    "    <img style=\"float: right;\" src='images/key_pts_expl.png' width=50% height=50%/>\n",
    " * `keypoints` is the list of K facial keypoints, stored in a torch tensor of shape (Kx2). We have K=15 keypoints that stand for:\n",
    "   * keypoints[0]: Center of the left eye\n",
    "   * keypoints[1]: Center of the right eye\n",
    "   * keypoints[2]: Left eye inner corner\n",
    "   * keypoints[3]: Left eye outer corner\n",
    "   * keypoints[4]: Right eye inner corner\n",
    "   * keypoints[5]: Right eye outer corner\n",
    "   * keypoints[6]: Left eyebrow inner end\n",
    "   * keypoints[7]: Left eyebrow outer end\n",
    "   * keypoints[8]: Right eyebrow inner end\n",
    "   * keypoints[9]: Right eyebrow outer end\n",
    "   * keypoints[10]: Nose tip\n",
    "   * keypoints[11]: Mouth left corner\n",
    "   * keypoints[12]: Mouth right corner\n",
    "   * keypoints[13]: Mouth center top lip\n",
    "   * keypoints[14]: Mouth center bottom lip\n",
    "   \n",
    "Each individual facial keypoint is represented by two coordinates (x,y) that specify the horizontal and vertical location of the keypoint respectively. All keypoint values are normalized to [-1,1], such that:\n",
    "   * (x=-1,y=-1) corresponds to the top left corner, \n",
    "   * (x=-1,y=1) to the bottom left corner,\n",
    "   * (x=1,y=-1) to the top right corner,\n",
    "   * (x=1,y=1) to the bottom right corner,\n",
    "   * and (x=0,y=0) to the center of the image.\n",
    "   \n",
    "      \n",
    "The data downloaded is already preprocessed and hence there is no need to apply transformations in order to prepare the data. Of course, feel free to apply training transformations to improve your performance such as e.g. flipping the training images. </br>\n",
    "\n",
    "NOTE: Do not forget that these transformations (data augmentation) are only applied to your training data. Your validation and test set remain untouched. </br>\n",
    "\n",
    "Also, when applying transformations such as flipping, make sure that the predicted coordinates of your keypoints change accordingly.\n",
    "\n",
    "Let's have a look at the first training sample to get a better feeling for the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 462,
     "status": "ok",
     "timestamp": 1656687937818,
     "user": {
      "displayName": "Dan Halperin",
      "userId": "04461491770918170797"
     },
     "user_tz": -120
    },
    "id": "-Uv8nLvcCPTn",
    "outputId": "51159487-0682-435f-b9c8-5135b69c1bb0"
   },
   "outputs": [],
   "source": [
    "image, keypoints = train_dataset[0][\"image\"], train_dataset[0][\"keypoints\"]\n",
    "print(\"Shape of the image:\", image.size())\n",
    "print(\"Smallest value in the image:\", torch.min(image))\n",
    "print(\"Largest value in the image:\", torch.max(image))\n",
    "print(image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1656687937818,
     "user": {
      "displayName": "Dan Halperin",
      "userId": "04461491770918170797"
     },
     "user_tz": -120
    },
    "id": "D28hhzk-CPTn",
    "outputId": "b4d05fa5-22f0-40c3-fb36-79ad18db05c0"
   },
   "outputs": [],
   "source": [
    "keypoints = train_dataset[0][\"keypoints\"]\n",
    "print(keypoints)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XLYOhkx7CPTo"
   },
   "source": [
    "In `exercise_code/util/vis_utils.py` we also provide you with a function `show_all_keypoints()` that takes in an image and keypoints and displays where the predicted keypoints are in the image. Let's use it to plot the first few images of our training set:\n",
    "\n",
    "**Note:** if your kernel dies when running the following cell, please uncomment the last line of the imports cell `os.environ['KMP_DUPLICATE_LIB_OK']='True'`and try it again. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 1536,
     "status": "ok",
     "timestamp": 1656687939350,
     "user": {
      "displayName": "Dan Halperin",
      "userId": "04461491770918170797"
     },
     "user_tz": -120
    },
    "id": "VQRWKbRCCPTo",
    "outputId": "02626bce-2b0f-477a-9006-074a9b073b6b"
   },
   "outputs": [],
   "source": [
    "def show_keypoints(dataset, num_samples=3):\n",
    "    for i in range(num_samples):\n",
    "        image = dataset[i][\"image\"]\n",
    "        key_pts = dataset[i][\"keypoints\"]\n",
    "        show_all_keypoints(image, key_pts)\n",
    "\n",
    "\n",
    "show_keypoints(train_dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mZap99KcCPTo"
   },
   "source": [
    "# 2. Facial Keypoint Detection\n",
    "Your task is to define and train a model for facial keypoint detection.\n",
    "\n",
    "The facial keypoint detection task can be seen as a regression problem, where the goal is to predict 30 different values that correspond to the 15 facial keypoint locations. Thus, we need to build a network that gets a (1x96x96) image as input and predicts 30 continuous outputs between [-1,1].\n",
    "\n",
    "## Dummy Model\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "    <h3>Task: Check Code</h3>\n",
    "    <p>In <code>exercise_code/networks/keypoint_nn.py</code> we defined a naive <code>DummyKeypointModel</code>, which always predicts the keypoints of the first training image in the dataset. Let's try it on a few images and visualize our predictions in red:\n",
    " </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1656687939580,
     "user": {
      "displayName": "Dan Halperin",
      "userId": "04461491770918170797"
     },
     "user_tz": -120
    },
    "id": "aQLwlDXjCPTo"
   },
   "outputs": [],
   "source": [
    "def show_keypoint_predictions(model, dataset, num_samples=3):\n",
    "    for i in range(num_samples):\n",
    "        image = dataset[i][\"image\"].to(device)\n",
    "        key_pts = dataset[i][\"keypoints\"].to(device)\n",
    "        predicted_keypoints = torch.squeeze(model(image).detach()).view(15, 2)\n",
    "        show_all_keypoints(image, key_pts, predicted_keypoints)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 1105,
     "status": "ok",
     "timestamp": 1656687940681,
     "user": {
      "displayName": "Dan Halperin",
      "userId": "04461491770918170797"
     },
     "user_tz": -120
    },
    "id": "IbbBJRmhCPTp",
    "outputId": "bd507cf0-c8b7-4dff-b65a-bfdb6c2e2ad7"
   },
   "outputs": [],
   "source": [
    "dummy_model = DummyKeypointModel()\n",
    "show_keypoint_predictions(dummy_model, train_dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eVKNbtcTCPTp"
   },
   "source": [
    "As we see, the model predicts the first sample perfectly, but for the remaining samples the predictions are quite off.\n",
    "\n",
    "## Loss and Metrics\n",
    "\n",
    "To measure the quality of the model's predictions, we will use the mean squared error (https://en.wikipedia.org/wiki/Mean_squared_error), summed up over all 30 keypoint locations. In PyTorch, the mean squared error is defined in `torch.nn.MSELoss()`, and we can use it like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 474,
     "status": "ok",
     "timestamp": 1656687941151,
     "user": {
      "displayName": "Dan Halperin",
      "userId": "04461491770918170797"
     },
     "user_tz": -120
    },
    "id": "CeoyyehrCPTp",
    "outputId": "81b6ef9e-80aa-4a3c-8d62-8570fddb64ff"
   },
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.MSELoss()\n",
    "for i in range(3):\n",
    "    image = train_dataset[i][\"image\"]\n",
    "    keypoints = train_dataset[i][\"keypoints\"]\n",
    "    predicted_keypoints = torch.squeeze(dummy_model(image)).view(15, 2)\n",
    "    loss = loss_fn(keypoints, predicted_keypoints)\n",
    "    print(\"Loss on image %d:\" % i, loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KrYEcXddCPTp"
   },
   "source": [
    "As expected, our dummy model achieves a loss close to 0 on the first sample, but on all other samples the loss is quite high.\n",
    "\n",
    "To obtain an evaluation score (in the notebook and on the submission server), we will use the following function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1208,
     "status": "ok",
     "timestamp": 1656687942353,
     "user": {
      "displayName": "Dan Halperin",
      "userId": "04461491770918170797"
     },
     "user_tz": -120
    },
    "id": "zC13aydNCPTq",
    "outputId": "016dca5b-43ae-4089-a390-0ce3b7c838b2"
   },
   "outputs": [],
   "source": [
    "def evaluate_model(model, dataset):\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    criterion = torch.nn.MSELoss()\n",
    "    dataloader = DataLoader(dataset, batch_size=1, shuffle=False)\n",
    "    loss = 0\n",
    "    for batch in dataloader:\n",
    "        image, keypoints = batch[\"image\"].to(device), batch[\"keypoints\"].to(device)\n",
    "        predicted_keypoints = model(image).view(-1, 15, 2).to(device)\n",
    "        loss += criterion(\n",
    "            torch.squeeze(keypoints), torch.squeeze(predicted_keypoints)\n",
    "        ).item()\n",
    "    return 1.0 / (2 * (loss / len(dataloader)))\n",
    "\n",
    "\n",
    "print(\"Score of the Dummy Model:\", evaluate_model(dummy_model, val_dataset))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lzbbMeE4CPTq"
   },
   "source": [
    "**To pass the assignment, you will need to achieve a score of at least 100**. As you can see, the score is calculated from the average loss, so **your average loss needs to be lower than 0.005**. Our dummy model only gets a score of around 60, so you will have to come up with a better model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4GCE6hqECPTr"
   },
   "source": [
    "## Step 1: Design your own model\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "    <h3>Task: Implement</h3>\n",
    "    <p> Now it is your turn to build your own model. To do so, you need to design a convolution neural network that takes images of size (Nx1x96x96) as input and produces outputs of shape (Nx30) in the range [-1,1]. Therefore, implement the <code>KeypointModel</code> class in <code>exercise_code/networks/keypoint_nn.py</code>.\n",
    "    </p>\n",
    "</div>\n",
    "\n",
    "Recall that CNN's are defined by a few types of layers:\n",
    "* Convolution layers\n",
    "* Max-pooling layers\n",
    "* Fully-connected layers\n",
    "\n",
    "You can design your network however you want, but we strongly suggest to include multiple convolution layers. You are also encouraged to use things like dropout and batch normalization to stabilize and regularize your network. If you want to build a really competitive model, have a look at some literature on keypoint detection, such as [this paper](https://arxiv.org/pdf/1710.00977.pdf).\n",
    "\n",
    "#### Define your model in the provided file \n",
    "`exercise_code/networks/keypoint_nn.py` file\n",
    "\n",
    "This file is mostly empty but contains the expected class name, and the methods that your model needs to implement (only `forward()` basically). You are also free to decide whether you want to use PyTorch Lightning or not.\n",
    "The only rules your model design has to follow are:\n",
    "* Inherit from either `torch.nn.Module` or `pytorch_lightning.LightningModule`\n",
    "* Perform the forward pass in forward(), predicting keypoints of shape (Nx30) for images of shape (Nx1x96x96)\n",
    "* Have less than 5 million parameters\n",
    "* Have a model size of less than 20MB after saving\n",
    "\n",
    "Furthermore, you need to pass all your hyperparameters to the model in a single dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1656687942354,
     "user": {
      "displayName": "Dan Halperin",
      "userId": "04461491770918170797"
     },
     "user_tz": -120
    },
    "id": "9tw7h1YtCPTr"
   },
   "outputs": [],
   "source": [
    "hparams = {\n",
    "    # TODO: if you have any model arguments/hparams, define them here\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GtMden_jCPTr"
   },
   "source": [
    "To test whether your model follows the basic rules, run the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 585,
     "status": "ok",
     "timestamp": 1656687942936,
     "user": {
      "displayName": "Dan Halperin",
      "userId": "04461491770918170797"
     },
     "user_tz": -120
    },
    "id": "zkFiXrhOCPTs",
    "outputId": "add2801f-fb10-49cf-b5c0-28c59dfb3c8c"
   },
   "outputs": [],
   "source": [
    "model = KeypointModel(hparams)\n",
    "test_keypoint_nn(model)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "Ja1w927OCPTs"
   },
   "source": [
    "## Step 2: Train your model\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "    <h3>Task: Implement</h3>\n",
    "    <p> In addition to the network itself, you will also need to write the code for the model training. You can use PyTorch Lightning for that, or you can also write it yourself in standard PyTorch.\n",
    "    </p>\n",
    "</div>\n",
    "\n",
    "**Hints:**\n",
    "* Use `torch.nn.MSELoss()` as loss function.\n",
    "\n",
    "* You have two options for training code:\n",
    "    - (Recommended) Use a straightforward training scheme. See 1.pytorch.ipynb from ex07.\n",
    "    - (Not Recommended, especially for colab user) Use the Trainer() class, to utilize the beauty of pytorch-lightning. See 3.pytorch_lightning.ipynb from ex07. NOTE(!!!): To prevent a common bug, send the dataloaders to the trainer, and not to the pytorch-lightning class itself.\n",
    "    - Don't call your model anything else besides \"model\", unless you notice that you'll need to modify the model name in the upcoming cells.\n",
    "    - You could refer to the training scheme from exercise_08 to complete your code. Understanding this pipeline is crucial for future works in deep learning.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 100251,
     "status": "ok",
     "timestamp": 1656688055600,
     "user": {
      "displayName": "Dan Halperin",
      "userId": "04461491770918170797"
     },
     "user_tz": -120
    },
    "id": "7fRJmnLnCPTs",
    "outputId": "87a13392-7be9-4296-e12b-d2d8c3878578"
   },
   "outputs": [],
   "source": [
    "########################################################################\n",
    "# TODO - Train Your Model                                              #\n",
    "########################################################################\n",
    "\n",
    "\n",
    "\n",
    "pass\n",
    "\n",
    "########################################################################\n",
    "#                           END OF YOUR CODE                           #\n",
    "########################################################################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RuNDv7kFCPTs"
   },
   "source": [
    "When you're done training, run the cells below to visualize some predictions of your model, and to compute a validation score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 1124,
     "status": "ok",
     "timestamp": 1656688135189,
     "user": {
      "displayName": "Dan Halperin",
      "userId": "04461491770918170797"
     },
     "user_tz": -120
    },
    "id": "W7-SOD-CCPTt",
    "outputId": "88844c59-737a-4139-9de6-c3e54cb994f5"
   },
   "outputs": [],
   "source": [
    "show_keypoint_predictions(model, val_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5848,
     "status": "ok",
     "timestamp": 1656688141265,
     "user": {
      "displayName": "Dan Halperin",
      "userId": "04461491770918170797"
     },
     "user_tz": -120
    },
    "id": "ksQY1k0iCPTt",
    "outputId": "1e96371b-d07b-4076-912f-321743f4dc91"
   },
   "outputs": [],
   "source": [
    "print(\"Score:\", evaluate_model(model, val_dataset))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fvOocX1FCPTt"
   },
   "source": [
    "# 3. Save Your Model for Submission\n",
    "\n",
    "If your model achieved a validation score of 100 or higher, save your model with the cell below and submit it to [the submission server](https://i2dl.vc.in.tum.de/). Your validation set is of course different from the test set on our server, so results may vary. Nevertheless, you will have a reasonable close approximation about your performance.\n",
    "\n",
    "Before that, we will check again whether the number of parameters is below 5 Mio. and the file size is below 20 MB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "executionInfo": {
     "elapsed": 357,
     "status": "ok",
     "timestamp": 1656688141613,
     "user": {
      "displayName": "Dan Halperin",
      "userId": "04461491770918170797"
     },
     "user_tz": -120
    },
    "id": "Df44Fc0zCPTt",
    "outputId": "3bd1c1ba-d750-436a-bf2c-998837b30ac6"
   },
   "outputs": [],
   "source": [
    "save_model(model, \"facial_keypoints.p\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fvcX9_h5CPTt"
   },
   "source": [
    "Congrats - you've now finished your first Convolution Neural Network! Simply run the following cell to create a zipped file for your implementations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8749,
     "status": "ok",
     "timestamp": 1656688150356,
     "user": {
      "displayName": "Dan Halperin",
      "userId": "04461491770918170797"
     },
     "user_tz": -120
    },
    "id": "w_QV7M1uCPTt",
    "outputId": "77b2de33-dbf0-4fe8-fedc-bc4ec7fd9bb0"
   },
   "outputs": [],
   "source": [
    "# Now zip the folder for upload\n",
    "from exercise_code.util.submit import submit_exercise\n",
    "\n",
    "submit_exercise(\"../output/exercise09\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jrl8SOl5CPTt"
   },
   "source": [
    "# Submission Goals\n",
    "\n",
    "- Goal: Implement and train a convolution neural network for facial keypoint detection.\n",
    "- Passing Criteria: Reach **Score >= 100** on __our__ test dataset. The submission system will show you your score after you submit.\n",
    "\n",
    "- Submission start: __January 12, 2023, 13.00__\n",
    "- Submission deadline: __January 18, 2023 - 15:59__ \n",
    "- You can make **$\\infty$** submissions until the deadline. Your __best submission__ will be considered for bonus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Exercise Review](https://docs.google.com/forms/d/e/1FAIpQLSedSHEJ5vN-9FmJN-IGSQ9heDM_8qJQjHL4glgQGlrpQJEYPQ/viewform?usp=pp_url&entry.999074405=Exercise+9:+Facial+Keypoints)\n",
    "We are always interested in your opinion. Now that you have finished this exercise, we would like you to give us some feedback about the time required to finish the submission and/or work through the notebooks. Please take the short time to fill out our [review form](https://docs.google.com/forms/d/e/1FAIpQLSedSHEJ5vN-9FmJN-IGSQ9heDM_8qJQjHL4glgQGlrpQJEYPQ/viewform?usp=pp_url&entry.999074405=Exercise+9:+Facial+Keypoints) for this exercise so that we can do better next time! :)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "1_facial_keypoints.ipynb",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "i2dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8 | packaged by conda-forge | (main, Nov 24 2022, 14:07:00) [MSC v.1916 64 bit (AMD64)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "54970da6898dad277dbf355945c2dee7f942d2a31ec1fc1455b6d4f552d07b83"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
